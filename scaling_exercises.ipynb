{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import env\n",
    "import wrangle\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "# np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record the lesson!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data wrangling\n",
    "df = wrangle.wrangle_telco()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in train, validate and test\n",
    "train, test = train_test_split(df, test_size = 0.2, random_state = 123)\n",
    "train, validate = train_test_split(train, test_size = 0.25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Apply the scalers we talked about in this lesson to your data and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thing\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "# Fit the thing\n",
    "scaler.fit(train[['monthly_charges']])\n",
    "\n",
    "#transform\n",
    "scaled = scaler.transform(train[['monthly_charges']])\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can make a new 'scaled' column in original dataframe if you wish\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply the .inverse_transform method to your scaled data. Is the resulting dataset the exact same as the original data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "scaled = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read the documentation for sklearn's QuantileTransformer. Use normal for the output_distribution and apply this scaler to your data. Visualize the result of your data scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of outliers on scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection function for accessing mysql \n",
    "def get_connection(db, user=env.user, host=env.host, password=env.password):\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'\n",
    "\n",
    "query = \"\"\"\n",
    "select * \n",
    "from properties_2017\n",
    "join predictions_2017 using(parcelid)\n",
    "where transactiondate between \"2017-05-01\" and \"2017-06-30\";\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, get_connection('zillow'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some prep\n",
    "df = df.rename(columns={\"bedroomcnt\": \"bedrooms\", \"bathroomcnt\": \"bathrooms\", \"calculatedfinishedsquarefeet\": \"square_feet\", \"taxamount\": \"taxes\", \"taxvaluedollarcnt\": \"tax_value\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"parcelid\",\n",
    "    \"bedrooms\",\n",
    "    \"bathrooms\",\n",
    "    \"square_feet\",\n",
    "    \"tax_value\"\n",
    "]\n",
    "\n",
    "df = df[features]\n",
    "df = df.set_index(\"parcelid\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the nulls\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in train, validate and test\n",
    "train, test = train_test_split(df, test_size = 0.2, random_state = 123)\n",
    "train, validate = train_test_split(train, test_size = 0.25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_outliers = train[train.tax_value <= 2_000_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "scaled = scaler.fit_transform(train[['tax_value']])\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = sklearn.preprocessing.MinMaxScaler()\n",
    "scaled1 = scaler.fit_transform(train_no_outliers[['tax_value']])\n",
    "scaled1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (18,5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.hist(train.tax_value, bins = 30)\n",
    "plt.title('Unscaled')\n",
    "# plt.xlim(-1,20)\n",
    "\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(scaled, bins = 30)\n",
    "plt.title('Min-Max with outliers')\n",
    "# plt.xlim(-1,20)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(scaled1)\n",
    "plt.title('Min-Max without outliers');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.RobustScaler()\n",
    "scaled = scaler.fit_transform(train[['tax_value']])\n",
    "\n",
    "scaler1 = sklearn.preprocessing.RobustScaler()\n",
    "scaled1 = scaler.fit_transform(train_no_outliers[['tax_value']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,5))\n",
    "\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.hist(train.tax_value, bins = 30)\n",
    "plt.title('Unscaled')\n",
    "# plt.xlim(-1,20)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(scaled, bins = 500)\n",
    "plt.title('Robust with outliers')\n",
    "plt.xlim(-1,5)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(scaled1, bins = 100)\n",
    "plt.title('Robust without outliers')\n",
    "\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.QuantileTransformer(output_distribution='normal')\n",
    "scaled = scaler.fit_transform(train[['tax_value']])\n",
    "\n",
    "scaler1 = sklearn.preprocessing.QuantileTransformer(output_distribution='normal')\n",
    "scaled1 = scaler.fit_transform(train_no_outliers[['tax_value']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,5))\n",
    "\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.hist(train.tax_value, bins = 30)\n",
    "plt.title('Unscaled')\n",
    "# plt.xlim(-1,20)\n",
    "\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(scaled, bins = 500)\n",
    "plt.title('Robust with outliers')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(scaled1, bins = 100)\n",
    "plt.title('Robust without outliers')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "\n",
    "1. Handle outliers first (unless you know that you want to use a non-linear (e.g. Quantile Transformer)) \n",
    "scaling\n",
    "\n",
    "\n",
    "2. MinMaxScaler will transform each value in the column proportionally within the desireable range (usually [0,1]). Use this as the your first choice to scale. It will preserve the shape of the distribution (no distortion).\n",
    "\n",
    "\n",
    "3. StandardScaler() will transform each value in the column to range about the mean 0 and standard deviation 1,  Use StandardScaler if you know the data distribution is normal.\n",
    "\n",
    "\n",
    "4. If there are outliers (which you don't want to discard), use RobustScaler(). \n",
    "    Alternatively you could remove the outliers and use either of the above 2 scalers\n",
    "\n",
    "\n",
    "5. Good practice to visualize the distribution of variables after scaling (make sure the transformation you were hoping for actually happened)\n",
    "\n",
    "\n",
    "6. Use non-linear scalers when you really have to (e.g.Quantiler Transformer when you must have data normally distributed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
